---
title: 'Goodness of Fit'
author: "Matteo Calgaro"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Goodness of Fit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEcoding{UTF-8}
  \usepackage[utf8]{inputenc}
bibliography: bib_gof.json
csl: genome-biology.csl
---

```{=html}
<style>
#TOC {
  top: 1%;
  opacity: 0.5;
}
#TOC:hover {
  opacity: 1;
}

</style>
```
```{r options, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning=FALSE, error=FALSE, message=FALSE)
```

# Installation

The recommended way to install the `benchdamic` package is

```{r, eval=FALSE}
devtools::install_github("mcalgaro93/benchdamic", build_vignettes = TRUE)
```

# Introduction

This vignette provides an introductory example on how to work with the "Goodness of Fit" analysis framework as proposed in [@calgaro2020].

As different methods rely on different statistical distributions to perform DA analysis, we assess the goodness of fit (GOF) of the statistical models underlying each method on a 16S and a WMS dataset. For each model, we evaluate its ability to correctly estimate the average counts and the proportion of zeroes by taxon.

We consider five distributions: (1) the negative binomial (NB) used in edgeR and DeSeq2 [@edger; @deseq2], (2) the zero-inflated negative binomial (ZINB) used in ZINB-WaVE [@zinbwave], (3) the truncated Gaussian Hurdle model of MAST [@mast], (4) the zero-inflated Gaussian (ZIG) mixture model of metagenomeSeq [@zig], and (5) the Dirichlet-Multinomial (DM) distribution underlying ALDEx2 [@aldex2].

First, let's load the packages.

```{r load_packs}
library(benchdamic)
# Datasets from bioconductor packages
library(HMP16SData)
library(curatedMetagenomicData)
# Data management
library(phyloseq)
library(plyr)
# Graphics
library(ggplot2)
library(cowplot)
```

# Data loading

For this step of the analysis we consider the count distributions for a homogeneous group of samples (e.g. only samples from a specific experimental condition, phenotype, treatment...).

In this specific example, datasets are downloaded from the `HMP16SData` and `curatedMetagenomicData` Bioconductor packages, however the user can use its own data. After the HMP 16S and WMS Stool samples download we make sure to extract the same biological samples from the datasets in both the technological applications.

We perform two filters:

1.  to Keep samples with library size bigger than $10^3$ in 16S and $10^6$ in WMS;

2.  to keep features with more than 10 counts in more than 1 samples.

```{r dataloading}
# WMS HMP data download
da
psWMS_Stool <- curatedMetagenomicData("HMP_2012.metaphlan_bugs_list.stool",
                                      dryrun = FALSE,
                                      counts = TRUE,
                                      bugs.as.phyloseq = TRUE)[[1]]
# Duplicated SubjectID removal
psWMS_Stool <- subset_samples(psWMS_Stool,!duplicated(subjectID))
# Filter on samples
psWMG_Stool <- prune_samples(sample_sums(psWMS_Stool) >= 10^6, psWMS_Stool)

## 16S HMP data download
ps16S_Stool <- V35() %>% 
  subset(select = HMP_BODY_SUBSITE == "Stool" & VISITNO == 1) %>% 
  as_phyloseq()
# Duplicated SubjectID removal
ps16S_Stool <- subset_samples(ps16S_Stool,!duplicated(RSID))
# Filter the samples
ps16S_Stool <- prune_samples(sample_sums(ps16S_Stool) >= 10^3, ps16S_Stool)
  
# SRS in both WMS and 16S
both_16S_WMS <- match(ps16S_Stool@sam_data$SRS_SAMPLE_ID,rownames(psWMS_Stool@sam_data))
ps16S <- subset_samples(ps16S_Stool, !is.na(both_16S_WMS))
psWMS <- subset_samples(psWMS_Stool, psWMS_Stool@sam_data$subjectID %in% paste0("HMP_2012_", ps16S@sam_data$RSID))
# Same sample order in both objects
psWMS@otu_table <- psWMS@otu_table[,ps16S@sam_data$SRS_SAMPLE_ID]
psWMS@sam_data <- psWMS@sam_data[ps16S@sam_data$SRS_SAMPLE_ID,]
  
Stool_WMS <- filter_taxa(psWMS,function(x) sum(x>10)>1,1)
Stool_16S <- filter_taxa(ps16S,function(x) sum(x>10)>1,1)

```

# Model estimation

## Negative Binomial and Zero-Inflated Negative Binomial Models

For any $\mu \ge 0$ and $\theta > 0$, let $f_{NB}(\cdot;\mu,\theta)$ denote the probability mass function (PMF) of the negative binomial (NB) distribution with mean $\mu$ and inverse dispersion parameter $\theta$, namely:$$
f_{NB} = \frac{\Gamma(y+\theta)}{\Gamma(y+1)\Gamma(\theta)}\left(\frac{\theta}{\theta+1} \right)^\theta\left(\frac{\mu}{\mu+\theta} \right)^y, \forall y \in \mathbb{N}
$$Note that another parametrization of the NB PMF is in terms of the dispersion parameter $\psi = \theta^{-1}$ (although $\theta$ is also sometimes called dispersion parameter in the literature). In both cases, the mean of the NB distribution is $\mu$ and its variance is:$$
\sigma^2 = \mu + \frac{\mu^2}{\theta} = \mu+\psi\mu^2
$$In particular, the NB distribution boils down to a Poisson distribution when $\psi=0 \iff \theta=+ \infty$.

For any $\pi\in[0,1]$, let $f_{ZINB}(\cdot;\mu,\theta,\pi)$ be the PMF of the ZINB distribution given by:

$$
f_{ZINB}(\cdot;\mu,\theta,\pi) = \pi\delta_0(y)+(1-\pi)f_{NB}(y;\mu,\theta), \forall y\in\mathbb{N}
$$

where $\delta_0(\cdot)$ is the Dirac function. Here, $\pi$ can be interpreted as the probability that a 0 is observed instead of the actual count, resulting in an inflation of zeros compared to the NB distribution, hence the name ZINB.

The packages we rely on to estimate these distributions on real count data are edgeR [@edger] and ZINB-WaVE [@zinbwave] but we can easily call the benchdamic functions `fitNB` and `fitZINB`.

## Zero-Inflated Gaussian Model

The raw count for sample j and feature i is denoted by $c_{ij}$. The zero-inflated model is defined for the continuity-corrected logarithm of the raw count data: $y_{ij} = log_2(c_{ij}+1)$ as a mixture of a point mass at zero $I_{0}(y)$ and a count distribution $f_{count}(y;\mu,\sigma^2) \sim N(\mu,\sigma^2)$. Given mixture parameters $\pi_j$, we have that the density of the ZIG distribution for feature i, in sample j with $s_j$ total counts is: $$f_{ZIG}(y_{ij};s_j,\beta,\mu_i,\sigma^2_i) = \pi_j(s_j)\cdot I_{0}(y_{ij})+(1-\pi_j(s_j))\cdot f_{count}(y_{ij};\mu,\sigma^2)$$

The mean model is specified as:$$E(y_{ij})=\pi_{j} + (1-\pi_j)\cdot\left(b_{i0}+\eta_ilog_2\left( \frac{s_j^\hat{l}}{N}+1 \right) \right)$$

In this case, parameter $b_{i0}$ is the intercept of the model while the term including the logged normalization factor $log_2\left(\frac{s_j^\hat{l}}{N}+1 \right)$ captures feature-specific normalization factors through parameter $\eta_i$. In details, $s_j^\hat{l}$ is the median scaling factor resulted from the Cumulative Sum Scaling (CSS) normalization procedure. $N$ is a constant fixed by default at 1000 but it should be a number close to the scaling factors to be used as a reference, for this reason a good choice could be the median of the scaling factors. The mixture parameters $\pi_j(s_j)$ are modeled as a binomial process:

$$log\frac{\pi_j}{1-\pi_j} = \beta_0+\beta_1\cdot log(s_j)$$

The package we rely on to estimate this distribution on real count data is metagenomeSeq [@zig] but we can easily call the benchdamic function `fitZIG`.

## Truncated Gaussian Hurdle Model

The original field of application of this method was the single-cell RNAseq data, where $y = log_2(TPM+1)$ expression matrix was modeled as a two-part generalized regression model [@mast]. In microbiome data that starting point translates to a $y_{ij} = log_2\left(counts_{ij}\cdot\frac{10^6}{libSize_{j}}+1 \right)$ or a $log_2\left(counts_{ij}\cdot\frac{ median(libSize)}{libSize_{j}}+1\right)$.

The taxon presence rate is modeled using logistic regression and, conditioning on a sample with the taxon, the transformed abundance level is modeled as Gaussian.

Given normalized, possibly thresholded, abundance $y_{ij}$, the rate of presence and the level of abundance for the samples were the taxon is present, are modeled conditionally independent for each gene $i$. Define the indicator $z_{ij}$, indicating whether taxon $i$ is expressed in sample $j$ (i.e., $z_{ij} = 0$ if $y_{ij} = 0$ and $z_{ij} = 1$ if $y_{ij} > 0$). We fit logistic regression models for the discrete variable $Z$ and a Gaussian linear model for the continuous variable $(Y|Z=1)$ independently, as follows:

$$ logit(Pr(Z_{ij}=1))=X_j\beta_i^D $$

$$ P(Y_{ij}=y|Z_{ij}=1) \sim N(X_j\beta^C_i,\sigma^2_i)$$

The package we rely on to estimate this distribution on real count data is MAST [@mast] but we can easily call the benchdamic function `fitHURDLE`.

## Dirichlet-Multinomial Mixture Model

The probability mass function of a $n$ dimensional multinomial sample $y = (y_1,...,y_n)^T$ with library size $libSize = \sum_{i=1}^ny_i$ and parameter $p=(p_1,...,p_n)$ is:

$$
f(y;p)= {libSize\choose y}\prod_{i=1}^np_i^{y_i}
$$

The mean-variance structure of the MN model doesn't allow over-dispersion, which is common in real data. DM distribution models the probability parameter $p$ in the MN model by a Dirichlet distribution. The probability mass of a n-category count vector $y$ over $libSize$ trials under DM with parameter $\alpha=(\alpha_1,...,\alpha_n)$, $a_i>0$ and proportion vector $p \in \Delta_n=\{(p_1,...,p_n):p_i\ge0,\sum_ip_i=1 \}$ is:

$$
f(y|\alpha)={libSize\choose y}\frac{\prod_{i=1}^n(a_i)y_i}{(\sum_i\alpha_i)\cdot libSize}
$$

The mean value for the $i^{th}$ taxon and $j^{th}$ sample of the count matrix is given by $libSize_j\cdot \frac{\alpha_{ij}}{\sum_i a_{ij}}$.

The package we rely on to estimate this distribution on real count data is MGML [@kim2018] but we can easily call the benchdamic function `fitDM`.

# Comparing Estimated and Observed counts

To easily compare estimated and observed mean values the natural logarithm transformation, with the continuity correction ($log(counts+1)$), is well suited, indeed it reduces count range making the differences more stable.

Except for `fitHURDLE`, which performs a CPM transformation on the counts (or the one with the median library size), and `fitZIG` which models the $log_2(counts+1)$, the other methods model the $counts$ directly. For these reasons, `fitHURDLE`'s output should not be compared directly to the observed $log(counts+1)$ mean values as for the other methods. Instead, the logarithm of the observed CPM (or the one with the median library size) should be used.

The function to prepare observed counts is `prepareObserved()`, specifying the `scale` parameter if the HURDLE model is considered. The function to compute mean differences (MD) and zero probability difference (ZPD) between estimated and observed values, is `meanDifferences()`.

A wrapper function to simultaneously perform the estimates and the mean differences is `fitModels()`. Using `ldply` function it is possible to arrange all the values from the `list`to a ready-to-plot `data.frame`.

### 16S Stool samples

```{r fitting_16S, warning=FALSE}
list_16S <- fitModels(counts = Stool_16S@otu_table@.Data,
                      models = c("NB","ZINB","DM","ZIG","HURDLE"))

df_16S <- plyr::ldply(list_16S,.id = "Model")
```

We obtain the RMSE values for MD values:

```{r RMSE_MD_16S}
RMSE_MD_16S <- plyr::ldply(list_16S,.fun = function(df) cbind("RMSE" = RMSE(df[,"MD"])),.id =  "Model")
RMSE_MD_16S
```

and for ZPD values:

```{r RMSE_ZPD_16S}
RMSE_ZPD_16S <- plyr::ldply(list_16S,.fun = function(df) cbind("RMSE" = RMSE(df[,"ZPD"])),.id =  "Model")
RMSE_ZPD_16S
```

To plot estimated and observed values we use the function `MDPlot`, based on `ggplot2`.

```{r plot16S, fig.width=15, fig.height=10}
cowplot::plot_grid(plotlist = list(MDPlot(data = df_16S,difference = "MD",split = FALSE),
MDPlot(data = df_16S,difference = "ZPD",split = FALSE)),
nrow = 2)
```

### WMS Stool samples

The same procedure can be followed for the WMS samples.

```{r fitting_WMS, warning=FALSE}
list_WMS <- fitModels(counts = Stool_WMS@otu_table@.Data,
                    models = c("NB","ZINB","DM","ZIG","HURDLE"))

df_WMS <- plyr::ldply(list_WMS,.id = "Model")
```

```{r plotWMS, fig.width=15, fig.height=10}
cowplot::plot_grid(plotlist = list(MDPlot(data = df_WMS,difference = "MD"),
MDPlot(data = df_WMS,difference = "ZPD")),
nrow = 2)
```

# References
