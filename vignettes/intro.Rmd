---
title: 'An introduction to benchdamic'
author: 
    - Matteo Calgaro
    - Chiara Romualdi
    - Davide Risso
    - Nicola Vitulo
output: 
  BiocStyle::html_document:
    toc: yes
    number_section: yes
    fig_caption: yes
vignette: >
  %\VignetteEcoding{UTF-8}
  \usepackage[utf8]{inputenc}
bibliography: bib_intro.json
csl: bioinformatics.csl
---

```{=html}
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Intro}
-->
```
```{r options, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, error = FALSE, message = FALSE, 
    dev = switch(output(), html = "svg", latex = "pdf"))
```

# Introduction

This vignette provides an introductory example on how to work with the analysis framework firstly proposed in [@calgaro].

The package is named benchdamic, acronym for "BENCHmarking of Differential Abundance detection methods for MICrobial data". Not only does the package structure allow the users to test a variety of commonly used methods for differential abundance analysis, but it also enables them to set benchmarks including custom methods on their datasets. Performances of each method are evaluated with respect to i) suitability of distributional assumptions, ii) ability to control false discoveries, iii) concordance of the findings, and iv) enrichment of differentially abundant microbial species in specific conditions. Each step of the assessment is flexible when it comes to the choice of differential abundance methods, their parameters, and input data types. Various graphic outputs lead the users to an informed decision when evaluating the most suitable method to use for their data.

# Installation

To install this package, start R (version "4.2") and enter:

```{r, eval=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("benchdamic")
```

Then, load some packages for basic functions and data.

```{r load_packs}
library(benchdamic)
# Generate simulated data
library(SPsimSeq)
# Data management
library(phyloseq)
library(SummarizedExperiment)
library(plyr)
# Graphics and tables
library(ggplot2)
library(cowplot)
```

# Goodness of Fit

## Data loading

In this demonstrative example, datasets are downloaded from the `HMP16SData` Bioconductor package [@HMP16SData]. We consider a homogeneous group of samples (e.g. only samples from a specific experimental condition, phenotype, treatment, body site...). See `help("ps_stool_16S")` for details.

```{r dataloading}
data("ps_stool_16S")
ps_stool_16S
```

## GOF structure

As different methods rely on different statistical distributions to perform DA analysis, we assess the goodness of fit (GOF) of the statistical models underlying some of the DA methods on a 16S dataset. For each model, we evaluate its ability to correctly estimate the average counts and the proportion of zeroes by taxon.

We consider five distributions: (1) the negative binomial (NB) used in edgeR and DeSeq2 [@edger; @deseq2], (2) the zero-inflated negative binomial (ZINB) used in ZINB-WaVE [@zinbwave], (3) the truncated Gaussian Hurdle model of MAST [@mast], (4) the zero-inflated Gaussian (ZIG) mixture model of metagenomeSeq [@zig], and (5) the Dirichlet-Multinomial (DM) distribution underlying ALDEx2 [@aldex2].

The relationships between the functions used in this section are explained by the diagram in Figure \@ref(fig:figGOF). To help with the reading: green boxes represent the inputs or the outputs, red boxes are the methods and blue boxes are the main parameters of those method.

```{r figGOF, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.cap="Goodness of Fit diagram."}
knitr::include_graphics("./GOF_structure.svg")
```

## Model estimation

### Negative Binomial and Zero-Inflated Negative Binomial Models

For any $\mu \ge 0$ and $\theta > 0$, let $f_{NB}(\cdot;\mu,\theta)$ denote the probability mass function (PMF) of the negative binomial (NB) distribution with mean $\mu$ and inverse dispersion parameter $\theta$, namely:$$
f_{NB} = \frac{\Gamma(y+\theta)}{\Gamma(y+1)\Gamma(\theta)}\left(\frac{\theta}{\theta+1} \right)^\theta\left(\frac{\mu}{\mu+\theta} \right)^y, \forall y \in \mathbb{N}
$$Note that another parametrization of the NB PMF is in terms of the dispersion parameter $\psi = \theta^{-1}$ (although $\theta$ is also sometimes called dispersion parameter in the literature). In both cases, the mean of the NB distribution is $\mu$ and its variance is:$$
\sigma^2 = \mu + \frac{\mu^2}{\theta} = \mu+\psi\mu^2
$$In particular, the NB distribution boils down to a Poisson distribution when $\psi=0 \iff \theta=+ \infty$.

For any $\pi\in[0,1]$, let $f_{ZINB}(\cdot;\mu,\theta,\pi)$ be the PMF of the ZINB distribution given by:

$$
f_{ZINB}(\cdot;\mu,\theta,\pi) = \pi\delta_0(y)+(1-\pi)f_{NB}(y;\mu,\theta), \forall y\in\mathbb{N}
$$

where $\delta_0(\cdot)$ is the Dirac function. Here, $\pi$ can be interpreted as the probability that a 0 is observed instead of the actual count, resulting in an inflation of zeros compared to the NB distribution, hence the name ZINB.

The packages we rely on to estimate these distributions on real count data are edgeR [@edger] and ZINB-WaVE [@zinbwave] but we can easily call the benchdamic functions `fitNB` and `fitZINB`.

### Zero-Inflated Gaussian Model

The raw count for sample j and feature i is denoted by $c_{ij}$. The zero-inflated model is defined for the continuity-corrected logarithm of the raw count data: $y_{ij} = log_2(c_{ij}+1)$ as a mixture of a point mass at zero $I_{0}(y)$ and a count distribution $f_{count}(y;\mu,\sigma^2) \sim N(\mu,\sigma^2)$. Given mixture parameters $\pi_j$, we have that the density of the ZIG distribution for feature i, in sample j with $s_j$ total counts is: $$f_{ZIG}(y_{ij};s_j,\beta,\mu_i,\sigma^2_i) = \pi_j(s_j)\cdot I_{0}(y_{ij})+(1-\pi_j(s_j))\cdot f_{count}(y_{ij};\mu,\sigma^2)$$

The mean model is specified as:$$E(y_{ij})=\pi_{j} + (1-\pi_j)\cdot\left(b_{i0}+\eta_ilog_2\left( \frac{s_j^{\hat{l}}}{N}+1 \right) \right)$$

In this case, parameter $b_{i0}$ is the intercept of the model while the term including the logged normalization factor $log_2\left(\frac{s_j^{\hat{l}}}{N}+1 \right)$ captures feature-specific normalization factors through parameter $\eta_i$. In details, $s_j^{\hat{l}}$ is the median scaling factor resulted from the Cumulative Sum Scaling (CSS) normalization procedure. $N$ is a constant fixed by default at 1000 but it should be a number close to the scaling factors to be used as a reference, for this reason a good choice could be the median of the scaling factors (which is used instead of 1000). The mixture parameters $\pi_j(s_j)$ are modeled as a binomial process:

$$log\frac{\pi_j}{1-\pi_j} = \beta_0+\beta_1\cdot log(s_j)$$

The package we rely on to estimate this distribution on real count data is metagenomeSeq [@zig] but we can easily call the benchdamic function `fitZIG`.

### Truncated Gaussian Hurdle Model

The original field of application of this method was the single-cell RNAseq data, where $y = log_2(TPM+1)$ expression matrix was modeled as a two-part generalized regression model [@mast]. In microbiome data that starting point translates to a $y_{ij} = log_2\left(counts_{ij}\cdot\frac{10^6}{libSize_{j}}+1 \right)$ or a $log_2\left(counts_{ij}\cdot\frac{ median(libSize)}{libSize_{j}}+1\right)$.

The taxon presence rate is modeled using logistic regression and, conditioning on a sample with the taxon, the transformed abundance level is modeled as Gaussian.

Given normalized, possibly thresholded, abundance $y_{ij}$, the rate of presence and the level of abundance for the samples were the taxon is present, are modeled conditionally independent for each gene $i$. Define the indicator $z_{ij}$, indicating whether taxon $i$ is expressed in sample $j$ (i.e., $z_{ij} = 0$ if $y_{ij} = 0$ and $z_{ij} = 1$ if $y_{ij} > 0$). We fit logistic regression models for the discrete variable $Z$ and a Gaussian linear model for the continuous variable $(Y|Z=1)$ independently, as follows:

$$ logit(Pr(Z_{ij}=1))=X_j\beta_i^D $$

$$ P(Y_{ij}=y|Z_{ij}=1) \sim N(X_j\beta^C_i,\sigma^2_i)$$

The package we rely on to estimate this distribution on real count data is MAST [@mast] but we can easily call the benchdamic function `fitHURDLE`.

### Dirichlet-Multinomial Mixture Model

The probability mass function of a $n$ dimensional multinomial sample $y = (y_1,...,y_n)^T$ with library size $libSize = \sum_{i=1}^ny_i$ and parameter $p=(p_1,...,p_n)$ is:

$$
f(y;p)= {libSize\choose y}\prod_{i=1}^np_i^{y_i}
$$

The mean-variance structure of the MN model doesn't allow over-dispersion, which is common in real data. DM distribution models the probability parameter $p$ in the MN model by a Dirichlet distribution. The probability mass of a n-category count vector $y$ over $libSize$ trials under DM with parameter $\alpha=(\alpha_1,...,\alpha_n)$, $a_i>0$ and proportion vector $p \in \Delta_n=\{(p_1,...,p_n):p_i\ge0,\sum_ip_i=1 \}$ is:

$$
f(y|\alpha)={libSize\choose y}\frac{\prod_{i=1}^n(a_i)y_i}{(\sum_i\alpha_i)\cdot libSize}
$$

The mean value for the $i^{th}$ taxon and $j^{th}$ sample of the count matrix is given by $libSize_j\cdot \frac{\alpha_{ij}}{\sum_i a_{ij}}$.

The package we rely on to estimate this distribution on real count data is MGLM [@MGLM; @MGLMpackage] but we can easily call the benchdamic function `fitDM`.

## Comparing Estimated and Observed counts

The goodness of fit for several distributions is assessed comparing estimated and observed values. On one side we can compare, for each taxon, the observed mean abundance with the estimated one, obtaining the Mean Difference (MD). On the other side we can compare the proportion of samples which have zero counts for a specific taxon with the estimated probability to observe a zero count, obtaining the Zero Probability Difference (ZPD).

To easily compare estimated and observed mean values the natural logarithm transformation, with the continuity correction ($log(counts+1)$), is well suited, indeed it reduces count range making the differences more stable.

Except for `fitHURDLE`, which performs a CPM transformation on the counts (or the one with the median library size), and `fitZIG` which models the $log_2(counts+1)$, the other methods, `fitNB`, `fitZINB`, and `fitDM`, model the $counts$ directly. For these reasons, `fitHURDLE`'s output should not be compared directly to the observed $log(counts+1)$ mean values as for the other methods. Instead, the logarithm of the observed CPM (or the one with the median library size) should be used.

```{r example_hurdle}
example_HURDLE <- fitHURDLE(
    object = ps_stool_16S,
    scale = "median"
)
head(example_HURDLE)
```

The values above are the `fitHURDLE` estimated values. Some *NA* values could be present due to taxa sparsity. The internally used function to prepare observed counts is `prepareObserved()`, specifying the `scale` parameter if the HURDLE model is considered.

```{r prepareObserved_hurdle}
observed_hurdle <- prepareObserved(
    object = ps_stool_16S, 
    scale = "median")
head(observed_hurdle)
```

Which are different from the non-scaled observed values:

```{r prepareObserved_normal}
head(prepareObserved(object = ps_stool_16S))
```

The function to compute mean differences (MD) and zero probability difference (ZPD) between estimated and observed values, is `meanDifferences()` (also used internally).

```{r meanDifferences_hurdle}
head(meanDifferences(
    estimated = example_HURDLE,
    observed = observed_hurdle
))
```

A wrapper function to simultaneously perform the estimates and the mean differences is `fitModels()`.

```{r fitting}
GOF_stool_16S <- fitModels(
    object = ps_stool_16S,
    models = c("NB", "ZINB", "DM", "ZIG", "HURDLE"),
    scale_HURDLE = c("median", "default"),
    verbose = FALSE # TRUE is always suggested
)
```

Exploiting the internal structure of the `fitModels`'s output we can obtain the Root Mean Squared Error (RMSE) values for MD values (the lower, the better):

```{r RMSE_MD}
plotRMSE(GOF_stool_16S, difference = "MD", plotIt = FALSE)
```

and for ZPD values:

```{r RMSE_ZPD}
plotRMSE(GOF_stool_16S, difference = "ZPD", plotIt = FALSE)
```

## Visualization

### Mean Differences

To plot estimated and observed values we use the function `plotMD`, based on `ggplot2` (Figure \@ref(fig:plotGOFMD)).

```{r plotGOFMD, fig.width=15, fig.height=4, fig.cap="MD plot. Mean-difference (MD) between the estimated and observed count values for each distribution."}
plotMD(
    data = GOF_stool_16S,
    difference = "MD",
    split = TRUE
)
```

If any warning messages are shown with this graph, they are likely due to sparse taxa. To address this we can inspect the number of NA values generated by each model (which are 24 for each HURDLE model):

```{r NA_values}
sapply(GOF_stool_16S, function(model) sum(is.na(model)))
```

From the plot in figure \@ref(fig:plotGOFMD) we would like to see no systematic trends for the mean difference values. Moreover, the closer the values are to the dotted line (representing equality between observed and estimated values), the better the goodness of fit relative to the model. To summarize the goodness of fit, the Root Mean Squared Error (RMSE) metric is also displayed for each model. For the *HURDLE_default* model, a quite different range of values of mean differences is displayed because of the excessive 'default' scaling proposed (1 million). It is also possible to plot only a subset of the estimated models (Figure \@ref(fig:plotGOFMDnoHurdleDefault)):

```{r plotGOFMDnoHurdleDefault, warning=FALSE, fig.width=15, fig.height=4, fig.cap="MD plot reduced. Mean-difference (MD) between the estimated and observed count values for the first 5 distributions."}
plotMD(
    data = GOF_stool_16S[1:5],
    difference = "MD",
    split = TRUE
)
```

From the Figure \@ref(fig:plotGOFMDnoHurdleDefault) we can see that *DM* distribution slightly overestimates the logarithm of the average counts for low values, while the *HURDLE_median* distribution presents an overestimation that increases as the observed values increase. *ZIG*, but especially *NB* and *ZINB* distributions produce very similar estimated and observed values. Similarly, to plot the mean differences for Zero Probability/Proportion we use the `plotMD` function (Figure \@ref(fig:plotGOFZPD)):

```{r plotGOFZPD, fig.width=15, fig.height=4, fig.cap="ZPD plot. Mean-difference between the estimated probability to observe a zero and the observed proportion of zero values (ZPD) for the first 5 distributions."}
plotMD(
    data = GOF_stool_16S[1:5],
    difference = "ZPD",
    split = TRUE
)
```

From the figure \@ref(fig:plotGOFZPD) we can see that *ZIG* and *NB* models underestimate the probability to observe a zero for sparse features, while the *HURDLE_median* model presents a perfect fit as the probability to observe a zero is the zero rate itself by construction. *DM* and *ZINB* models produce estimated values very similar to the observed ones. MDs and ZPDs are also available in the Figure \@ref(fig:plotGOFMDcollapsed) with a different output layout:

```{r plotGOFMDcollapsed, warning=FALSE, fig.width=10, fig.height=5, fig.cap="MD and ZPD plots. MD and ZPD plotted together for the first 5 distributions."}
plot_grid(plotMD(data = GOF_stool_16S[1:5], difference = "MD", split = FALSE),
    plotMD(data = GOF_stool_16S[1:5], difference = "ZPD", split = FALSE),
    ncol = 2
)
```

### RMSE

As already mentioned, to summarize the goodness of fit, the Root Mean Squared Error (RMSE) metric is used. The summary statistics for the overall performance are visible in Figure \@ref(fig:plotGOFRMSE):

```{r plotGOFRMSE, fig.width=10,fig.height=7, fig.cap="RMSE plot. Root Mean Squared Errors (RMSE) for both the MD and ZPD values for all the distributions."}
plot_grid(plotRMSE(GOF_stool_16S, difference = "MD"),
    plotRMSE(GOF_stool_16S, difference = "ZPD"),
    ncol = 2
)
```

The lower the RMSE value, the better the goodness of fit of the model.

## Discussion about GOF

The Goodness of Fit chapter is focused on some existing parametric models: NB, ZINB, HURDLE, ZIG, DM. The assumption of this analysis is that if a model estimates the data well, then a method based on that model may be a possibly good choice for studying the differential abundance. Other distributions could also be investigated (Poisson, Zero-Inflated Poisson...) but what about DA methods which are based on non-parametric models such as ANCOM? We can't use the GOF framework to compare the parametric models to non-parametric models. However, non-parametric methods may work well in real scenarios due to their added robustness and other evaluations are necessary in order not to favor one group of methods over another.

# Type I Error Control

We next try to evaluate Type I Error rate Control of each differential abundance detection method, i.e., the probability of the statistical test to call a feature DA when it is not. To do so, we consider mock comparisons on HMP Stool samples in which no true DA is present. Briefly, we randomly assign each sample to one of two experimental groups and perform DA analysis between these groups, repeating the process 10 times (at least 1000 times are suggested). In this setting, the p-values of a perfect test should be uniformly distributed between 0 and 1 and the false positive rate (FPR or observed $\alpha$), which is the observed proportion of significant tests, should match the nominal value (e.g., $\alpha=0.05$).

## TIEC structure

The Type I Error Control (TIEC) analysis is summarized by the following diagram. The main functions are the `createMocks()` which randomly generates the mock groups, `runMocks()` which relies on `runDA()` function and performs the DA analysis on mock comparisons, and `createTIEC()` which produces the data.frame for plotting the results.

The relationships between the functions used in this section are explained by the diagram in Figure \@ref(fig:figTIEC).

```{r figTIEC, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.cap="Type I Error Control diagram."}
knitr::include_graphics("./TIEC_structure.svg")
```

Where the DA framework decided by the user is composed by two main steps:

-   normalization (optional, depending on which methods are included);
-   differential abundance.

These steps can be performed both directly, using the `norm_*()` and `DA_*()` methods for normalization and differential abundance respectively, or indirectly, using the `setNormalizations()` and `set_*()` functions. This allows a higher flexibility to the users who can set the instructions for normalization or DA analysis only at the beginning. If some modifications are needed, the users can re-set the methods or modify the list of instructions directly. A list of the available methods is presented below (Table \@ref(tab:availableMethodsTable)). Some basic information are reported for each DA method, for more details please refer to functions' manual.

```{r availableMethodsTable, echo=FALSE}
if(!require("kableExtra", quietly = TRUE)){
    message("Please install 'kableExtra' to plot the methods.")
} else {
    available_methods <- read.csv(file = "./benchdamic_methods.csv", sep = ";")
    kableExtra::kable(
        x = available_methods, 
        caption = "DA methods available in benchdamic.",
        col.names = c("Method (package)", 
            "Short description", "Test",
            "Normalization / Transformation", 
            "Suggested input", "Output"), 
        booktabs = TRUE) %>%
        kable_styling(latex_options = "scale_down") %>%
        row_spec(0, bold = TRUE, color = "black") %>%
        column_spec(c(1,5,6), width = "3cm") %>% 
        column_spec(2:4, width = "6cm") %>%
        column_spec(1:6, color = "black")
}

```

## Create mock comparisons

Using `createMocks()` function we can randomly group the samples, `N = 2` times. A higher `N` is suggested (at least 1000) but in that case a longer running time is required.

```{r createMocks}
set.seed(123)
my_mocks <- createMocks(
    nsamples = phyloseq::nsamples(ps_stool_16S),
    N = 2
) # At least N = 1000 is suggested
```

## Differential abundance

Once the mocks have been generated, we perform DA analysis. Firstly, we add to the `phyloseq` object (or `TreeSummarizedExperiment` object) some scaling factors, such as *TMM* from `edgeR` and *CSS* from `metagenomeSeq`, and some normalization factor such as *poscounts* from `DESeq2`. This can be done, manually, using the `norm_edgeR()`, `norm_DESeq2()`, and `norm_CSS()` methods:

```{r normalizationManual, eval=FALSE}
ps_stool_16S <- norm_edgeR(
    object = ps_stool_16S,
    method = "TMM"
)
ps_stool_16S <- norm_DESeq2(
    object = ps_stool_16S,
    method = "poscounts"
)
ps_stool_16S <- norm_CSS(
    object = ps_stool_16S,
    method = "CSS"
)
```

Or, more fastly, using the `setNormalizations()` and `runNormalizations()` methods:

```{r setNormalization}
my_normalizations <- setNormalizations(
    fun = c("norm_edgeR", "norm_DESeq2", "norm_CSS"), 
    method = c("TMM", "poscounts", "CSS"))
ps_stool_16S <- runNormalizations(normalization_list = my_normalizations, 
    object = ps_stool_16S, verbose = TRUE)
```

Some messages "Found more than one"phylo" class in cache..." could be shown after running the previous functions. They are caused by duplicated class names between 'phyloseq' and 'tidytree' packages and can be ignored. After the normalization/scaling factors have been added to the phyloseq or TreeSummarizedExperiment object, the user could decide to filter rare taxa which do not carry much information. In this example vignette a simple filter is applied to keep only features with a count in at least 3 samples.

```{r filter_rare_taxa}
ps_stool_16S <- phyloseq::filter_taxa(
    physeq = ps_stool_16S, 
    flist = function(x) sum(x > 0) >= 3, prune = TRUE)
ps_stool_16S
```

We also compute the zero-inflated negative binomial weights using the `weights_ZINB` function.

```{r weights}
zinbweights <- weights_ZINB(
    object = ps_stool_16S,
    K = 0,
    design = "~ 1"
)
```

For each row of the `mock_df` data frame we run a bunch of DA methods. In this demonstrative example we use:

-   basic *t* and *wilcox* tests;

-   `edgeR` with *TMM* scaling factors [@edger] with and without *ZINB* weights [@zinbwave; @zinbweights];

-   `DESeq2` with *poscounts* normalization factors [@deseq2] with and without *ZINB* weights [@zinbwave; @zinbweights];

-   `limma-voom` with *TMM* scaling factors [@limma; @voom; @limmarnaseq] with and without *ZINB* weights [@zinbwave; @zinbweights];

-   `ALDEx2` with *all* and *iqlr* data transformation (denom parameter) performing the wilcox test [@aldex2];

-   `metagenomeSeq` with *CSS* normalization factors using both the *fitFeatureModel* (for a zero-inflated, log-normal distribution, mixture model, as suggested in the package vignette) and the *fitZig* (for a zero-inflated, gaussian distribution, mixture model) algorithms [@zig];

-   `corncob` with a focus on average differences (not dispersion, regulated by phi.formula and phi.formula_null parameters) using both *Wald* and *LRT* tests [@corncob];

-   `MAST` with both rescalings, *default* (*i.e.* $10^6$, for CPMs) and *median* [@mast];

-   `Seurat` with *LogNormalize* and *CLR* normalization/transformations, *t* and *wilcox* tests, and $10^5$ as scaling factor [@seurat];

-   `ANCOM` based on ANCOM-II algorithm with sampling fraction bias correction (BC parameter) [@ancom-bc; @ancom-ii];

-   `dearseq` with *permutation* and *asymptotic* tests [@dearseq];

Among the available methods, `NOISeq` [@noiseq] has not been used since it does not return p-values but only adjusted ones. Many combination of parameters are still possible for all the methods.

```{r set_Methods}
my_basic <- set_basic(pseudo_count = FALSE,
    contrast = c("group", "grp2", "grp1"), 
    test = c("t", "wilcox"), 
    paired = FALSE, 
    expand = TRUE)

my_edgeR <- set_edgeR(
    pseudo_count = FALSE,
    group_name = "group",
    design = ~ group,
    robust = FALSE,
    coef = 2,
    norm = "TMM",
    weights_logical = c(TRUE, FALSE),
    expand = TRUE)

my_DESeq2 <- set_DESeq2(
    pseudo_count = FALSE,
    design = ~ group,
    contrast = c("group", "grp2", "grp1"),
    norm = "poscounts",
    weights_logical = c(TRUE, FALSE),
    alpha = 0.05,
    expand = TRUE)

my_limma <- set_limma(
    pseudo_count = FALSE,
    design = ~ group,
    coef = 2,
    norm = "TMM",
    weights_logical = c(FALSE, TRUE),
    expand = TRUE)

my_ALDEx2 <- set_ALDEx2(
    pseudo_count = FALSE,
    design = "group",
    mc.samples = 128,
    test = "wilcox",
    paired.test = FALSE, 
    denom = c("all", "iqlr"), 
    contrast = c("group", "grp2", "grp1"),
    expand = TRUE)

my_metagenomeSeq <- set_metagenomeSeq(
    pseudo_count = FALSE, 
    design = "~ group", 
    coef = "groupgrp2", 
    norm = "CSS", 
    model = c("fitFeatureModel", "fitZig"),
    expand = TRUE)

my_corncob <- set_corncob(
    pseudo_count = FALSE,
    formula = ~ group,
    formula_null = ~ 1,
    phi.formula = ~ group,
    phi.formula_null = ~ group,
    test = c("Wald", "LRT"),
    boot = FALSE, 
    coefficient = "groupgrp2")

my_MAST <- set_MAST(
    pseudo_count = FALSE,
    rescale = c("default", "median"),
    design = "~ 1 + group",
    coefficient = "groupgrp2",
    expand = TRUE)

my_Seurat <- set_Seurat(
    pseudo_count = FALSE,
    test = c("t", "wilcox"),
    contrast = c("group", "grp2", "grp1"),
    norm = c("LogNormalize", "CLR"),
    scale.factor = 10^5,
    expand = TRUE
)

my_ANCOM <- set_ANCOM(
    pseudo_count = FALSE,
    formula = "group", 
    contrast = c("group", "grp2", "grp1"),
    BC = TRUE, 
    expand = TRUE   
)

my_dearseq <- set_dearseq(
    pseudo_count = FALSE,covariates = NULL,
    variables2test = "group", 
    preprocessed = FALSE, 
    test = c("permutation", "asymptotic"),
    expand = TRUE)

my_methods <- c(my_basic, my_edgeR, my_DESeq2, my_limma, my_metagenomeSeq,
    my_corncob, my_ALDEx2, my_MAST, my_Seurat, my_ANCOM, my_dearseq)
```

```{r runMocks}
# Random grouping each time
Stool_16S_mockDA <- runMocks(mocks = my_mocks, method_list = my_methods, 
    object = ps_stool_16S, weights = zinbweights, verbose = FALSE)
```

If some warnings are reported set `verbose = TRUE` to obtain the method name and the mock comparison where the warnings occured.

The structure of the output in this example is the following:

-   *Comparison1* to *Comparison2* on the first level, which contains:

    -   *Method1* to *Method23* output lists on the second level:

        -   `pValMat` which contains the matrix of raw p-values and adjusted p-values in *rawP* and *adjP* columns respectively;

        -   `statInfo` which contains the matrix of summary statistics for each feature, such as the *logFC*, standard errors, test statistics and so on;

        -   `dispEsts` which contains the dispersion estimates for methods like *edgeR* and *DESeq2*;

        -   `name` which contains the complete name of the used method.

Be sure to create a similar result structure (respecting the names and types of the objects) if a custom method is added: `dispEsts` vector is optional, while the matrix `pValMat`, and the character `name` are mandatory for the "Type I Error Control" results' visualization (see the example below for more information). `statInfo` matrix is also very important for results visualization, however it will be more useful later when the direction of the differential abundance will be investigated.

```{r customExample, eval=FALSE}
DA_yourMethod <- function(object, parameters) # others
{
    ### your method code 

    ### extract important statistics 
    vector_of_pval <- NA # contains the p-values
    vector_of_adjusted_pval <- NA # contains the adjusted p-values
    name_of_your_features <- NA # contains the OTU, or ASV, or other feature
                                # names. Usually extracted from the rownames of
                                # the count data
    vector_of_logFC <- NA # contains the logFCs
    vector_of_statistics <- NA # contains other statistics

    ### prepare the output 
    pValMat <- data.frame("rawP" = vector_of_pval,
                          "adjP" = vector_of_adjusted_pval)
    statInfo <- data.frame("logFC" = vector_of_logFC,
                           "statistics" = vector_of_statistics)
    name <- "write.here.the.name"
    # Be sure that your method hasn't changed the order of the features. If it
    # happens, you'll need to re-establish the original order.
    rownames(pValMat) <- rownames(statInfo) <- name_of_your_features

    # Return the output as a list
    return(list("pValMat" = pValMat, "statInfo" = statInfo, "name" = name))
} # END - function: DA_yourMethod
```

Once the custom method is set, it is very simple to add it to the framework:

-   direct way - just run the method using the function `DA_yourMethod()`;

-   indirect way - create a list containing one or more instances of the custom method with the desired combination of parameters and add it to the `my_methods` list.

```{r customExampleInstances, eval=FALSE}
my_custom_method <- list(
    customMethod.1 = list(method = "DA_yourMethod", parameters),
    customMethod.2 = list(method = "DA_yourMethod", parameters)
)
```

In this case, the 'method' field, containing the name of the method to call, is mandatory. Instead, the argument containing the data object is not needed in order to keep the `my_custom_method` object more lightweight. To run the methods:

```{r customExampleRun, eval=FALSE}
# Add the custom method instances to the others
my_methods <- c(my_edgeR, my_DESeq2, my_limma, my_custom_method)
# Run all the methods on a specific data object...
runDA(my_methods = my_methods, object = dataObject)
# ... Or on the mock datasets to investigate TIEC
runMocks(mocks = mock_df, my_methods = my_methods, object = dataObject)
```

It may happen that at a later time the user wants to add to the results already obtained, the results of another group of methods. First of all, the same mocks and the same object must be used to obtain the new results:

```{r new_methods}
my_new_limma <- set_limma(
    pseudo_count = FALSE,
    design = ~ group,
    coef = 2,
    norm = "CSS",
    weights_logical = FALSE)
```

Which returns a new set of `limma` instructions and a warning for using CSS normalization factors instead of those native to `edgeR`. We can run the new instructions:

```{r runMocks_new_limma}
Stool_16S_mockDA_new_limma <- runMocks(mocks = my_mocks, 
    method_list = my_new_limma, object = ps_stool_16S, verbose = FALSE)
```

To put everything together we can use a simple `mapply` and exploit the output structures:

```{r merging_old_and_new}
Stool_16S_mockDA_merged <- mapply(
    Stool_16S_mockDA, 
    Stool_16S_mockDA_new_limma, 
    FUN = function(old, new){
        c(old, new)
    }, SIMPLIFY = FALSE)
```

## Visualization

Since the visualization rely on `ggplot2` package, firstly we run the `createTIEC()` function of the `benchdamic` package, in order to produce a list of 4 `data.frames`:

1.  `df_pval` is a 5 columns and *number_of_features x methods x comparisons* rows `data.frame`. The four columns are called *Comparison*, *Method*, *variable* (which contains the feature names), *pval* and *padj*;

2.  `df_FPR` is a 5 columns and *methods x comparisons* rows `data.frame`. For each set of method and comparison, the proportion of false positives, considering 3 threshold (0.01, 0.05, 0.1) are reported;

3.  `df_QQ` contains the average p-value for each theoretical quantile, i.e. the coordinates to draw the QQ-plot for comparing the mean observed p-value distribution across comparisons, with the theoretical uniform distribution. Indeed, the observed p-values should follow a uniform distribution under the null hypothesis of no differential abundant features presence;

4.  `df_KS` is a 5 columns and *methods x comparisons* rows `data.frame`. For each set of method and comparison, the Kolmogorov-Smirnov test statistics and pvalues are reported in *KS* and *KS_pval* columns respectively.

```{r createTIEC, warning=FALSE}
TIEC_summary <- createTIEC(Stool_16S_mockDA)
```

### False Positive Rate

The false positive rate (FPR or observed $\alpha$), which is the observed proportion of significant tests, should match the nominal value because all the findings are false positive by construction. In this example `edgeR.TMM`, `edgeR.TMM.weighted`, `limma.TMM.weighted`, and `metagenomeSeq.CSS.fitZig` appear to be quite over all the thresholds, differently `ALDEx2.all.wilcox.unpaired` and `basic_t` methods are below the thresholds (Figure \@ref(fig:FPRplot)).

```{r FPRplot, fig.cap="FPR plot. Boxplots of the proportion of raw p-values lower than the commonly used thresholds for the nominal $\\alpha$ (i.e. the False Positive Rate) for each DA method.", fig.width=8, fig.height=5}
cols <- createColors(variable = levels(TIEC_summary$df_pval$Method))
plotFPR(df_FPR = TIEC_summary$df_FPR, cols = cols)
```

### QQ-Plot

The p-value distribution under the null hypothesis should be uniform. This is qualitatively summarized in the QQ-plot in Figure \@ref(fig:QQplot) where the bisector represents a perfect correspondence between observed and theoretical quantiles of p-values. For each theoretical quantile, the corresponding observed quantile is obtained averaging the observed p-values' quantiles from all 10 mock datasets. The plotting area is zoomed-in to show clearly the area between 0 and 0.1.

Methods over the bisector show a conservative behavior, while methods below the bisector a liberal one.

The starting point is determined by the total number of features. In our example the starting point for the theoretical p-value is computed as 1 divided by the number of taxa, rounded to the second digit. In real experiments, where the number of taxa is higher, the starting point is closer to zero.

```{r QQplot, fig.cap="QQ plot. Quantile-quantile plot from 0 to 0.1 for each DA methods. Average curves are reported", fig.width=7, fig.height=5}
plotQQ(df_QQ = TIEC_summary$df_QQ, zoom = c(0, 0.1), cols = cols) + 
    guides(colour = guide_legend(ncol = 1))
```

As the number of methods increases, the difficulty of distinguishing their curves increases. For this reason it is also possible to plot each method singularly (Figure \@ref(fig:QQplotsplit)).

```{r QQplotsplit, fig.cap="QQ plot. Quantile-quantile plots from 0 to 1 for each DA method are displayed separately. Average curves are reported", fig.height=10, fig.width=10}
plotQQ(df_QQ = TIEC_summary$df_QQ, zoom = c(0, 1), cols = cols, split = TRUE)
```

### Kolmogorov-Smirnov test

Departure from uniformity is evaluated through the Kolmogorov-Smirnov test which is reported for each method across all mock datasets using the the `plotKS` function in Figure \@ref(fig:KSplot).

```{r KSplot, fig.cap="KS plot. Kolmogorov-Smirnov (KS) statistic boxplots for each DA methods where the raw p-value distribution is compared with a uniform distribution."}
plotKS(df_KS = TIEC_summary$df_KS, cols = cols)
```

High KS values indicates departure from the uniformity while low values indicates closeness. All the clues we had seen in the previous figures \@ref(fig:QQplot) and \@ref(fig:QQplotsplit) are confirmed by the KS statistics: `metagenomeSeq.CSS.fitZig`, which was very liberal and its distribution of p-values is the farthest from uniformity among the tested methods. Also ALDEx2 based methods show high KS values, indeed they showed a very conservative behaviour.

### Log distribution of p-values

Looking at the p-values' log-scale can also be informative. This is because behavior in the tail may be poor even when the overall p-value distribution is uniform, with a few unusually small p-values in an otherwise uniform distribution. Figure \@ref(fig:LogPplot) displays the distributions of all the p-values (in negative log scale) generated by each DA method across all the mock comparisons.

```{r LogPplot, fig.cap="-log10(p-value) plot. Negative logarithm distribution of p-values. Red-shaded vertical bars represent the 90, 95, and 99 percentiles for the negative log distribution of p-values for each method. They should align with the dotted lines which represent the percentiles of the IDEAL distribution.", fig.width=7, fig.height=7}
plotLogP(df_pval = TIEC_summary$df_pval, cols = cols)
```

Similarly, figure \@ref(fig:AveLogPplot) exploits the structure of the `df_QQ` data.frame generated by the `createTIEC()` function to display the distribution of the p-values (in negative log scale) generated by each DA method, averaged among mock comparisons (only two in this vignette). As this second graphical representation is only based on 1 averaged p-value for each quantile, it is also less influenced by anomalously large values.

```{r AveLogPplot, fig.cap="-log10(average p-value) plot ('average' refers to the average p-value computed for each quantile across mocks comparisons). Negative logarithm distribution of average p-values. Red-shaded vertical bars represent the 90, 95, and 99 percentiles for the negative log distribution of average p-values for each method. They should align with the dotted lines which represent the percentiles of the IDEAL distribution.", fig.width=7, fig.height=7}
plotLogP(df_QQ = TIEC_summary$df_QQ, cols = cols)
```

In the figure \@ref(fig:LogPplot) and \@ref(fig:AveLogPplot), the $-\log_{10}(p-value)$ IDEAL distribution is reported in red color as the first method. To highlight tail's behaviors, 3 percentiles (0.9, 0.95, 0.99) are reported using red-shaded vertical segments for each method. If the method's distribution of negative log-transformed p-values or average p-values is still uniform in the 3 selected quantiles of the tail, the 3 red vertical segments will align to the respective dotted line. Methods are ordered using the distances between the observed quantiles and the ideal ones. Usually, when a method has its red segments to the left of the IDEAL's (e.g. `ALDEx2.iqlr.wilcox.unpaired` and `MAST.default`) its behavior is conservative. Indeed, for those methods, little p-values are fewer than expected. On the contrary, methods with red segments to the right of the IDEAL's (e.g. `edgeR.TMM`) have a liberal behavior. Mixed results could be present: a method that has a lower quantile for one threshold, while higher quantiles for the others (e.g. `limma.TMM`).

## Discussion about TIEC

Putting all the previous graphical representations together gives us a general overview of methods' ability to control false positives and p-values distribution under the null hypothesis (i.e. no differential abundance). It is clear that only methods that produce p-values can be included in this analysis. While figures \@ref(fig:QQplot) and \@ref(fig:QQplotsplit) have a main exploratory scope regarding the p-value distribution based on quantile-quantile comparison, figures \@ref(fig:FPRplot), \@ref(fig:KSplot), and \@ref(fig:AveLogPplot) are able to rank methods according to False Positive Rate, uniformity of p-value distribution, and departure from uniformity distribution in the tail. The latter graphical representations could be used as a first tool to establish which DA method to consider for further analyses and which DA methods to exclude.

# Concordance

If we want to measure the ability of each method to produce replicable results in independent data and the amount of concordance of the results of each method on two random subsets of the same dataset, we'll perform the Between Method Concordance (BMC) and the Within Method Concordance (WMC) analyses.

We consider the samples from the supragingival and subgingival plaques of 30 subjects. See `help("ps_plaque_16S")` for details.

```{r dataloading_concordance}
data("ps_plaque_16S")
ps_plaque_16S
```

## Concordance structure

Several functions are involved in the concordance analysis. First of all, the normalization methods need to be set up using the `setNormalizations()` function. The differential abundance framework is set up by using `set_edgeR()`, `set_DESeq2()`, `set_limma()`, and so on, functions. Once all these instructions are set up, the user can call the `runSplits()` which relies on `runNormalizations()` and `runDA()` functions. Finally, the `createConcordance()` and `plotConcordance()` functions are used to compute and plot the results.

The relationships between the functions used in this section are explained by the diagram in Figure \@ref(fig:figconcordance).

```{r figconcordance, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.cap="Concordance diagram."}
knitr::include_graphics("./concordance_structure.svg")
```

## Split datasets

Using the `createSplits()` function, the dataset is randomly divided by half. In this particular demonstrative dataset, samples are paired: 1 sample for supragingival plaque and 1 sample for subgingival plaque are considered for each subject. The *paired* parameter is passed to the method (it contains the name of the variable which describes the subject IDs) so the paired samples are inside the same split. In this specific case, the two groups of samples are balanced between conditions, reflecting the starting dataset. However, if the starting dataset had been unbalanced, the *balanced* option would have allowed to keep the two splits unbalanced or not.

```{r createSplits}
set.seed(123)
sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE <- 
    factor(sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE)
sample_data(ps_plaque_16S)$RSID <- 
    factor(sample_data(ps_plaque_16S)$RSID)

my_splits <- createSplits(
  object = ps_plaque_16S,
  varName = "HMP_BODY_SUBSITE",
  paired = "RSID",
  balanced = TRUE,
  N = 2
) # At least 100 is suggested
```

The structure produced by `createSplits()` function consists in a list of two matrices: *Subset1* and *Subset2*. Each matrix contains the randomly chosen sample IDs. The number of rows of both matrices is equal to the number of comparisons/splits (2 in our example, but at least 100 are suggested).

## Differential abundance

For some of the methods implemented in this package it is possible to perform differential abundance testings for the repeated measurements experimental designs (e.g. by adding the subject ID in the model formula of `DESeq2`).

Once again, to set the differential abundance methods to use, the `set_*()` methods can be exploited. For a faster demonstration, differential abundance methods without weighting are used:

```{r set_Methods_noweights}
my_edgeR_noWeights <- set_edgeR(
    group_name = "HMP_BODY_SUBSITE", 
    design = ~ 1 + RSID + HMP_BODY_SUBSITE, 
    coef = "HMP_BODY_SUBSITESupragingival Plaque", 
    norm = "TMM")

my_DESeq2_noWeights <- set_DESeq2(
    contrast = c("HMP_BODY_SUBSITE",
    "Supragingival Plaque", "Subgingival Plaque"), 
    design = ~ 1 + RSID + HMP_BODY_SUBSITE, 
    norm = "poscounts")

my_limma_noWeights <- set_limma(
    design = ~ 1 + RSID + HMP_BODY_SUBSITE, 
    coef = "HMP_BODY_SUBSITESupragingival Plaque", 
    norm = "TMM")

my_ALDEx2 <- set_ALDEx2(
    pseudo_count = FALSE,
    design = "HMP_BODY_SUBSITE",
    mc.samples = 128,
    test = "wilcox",
    paired.test = TRUE, 
    denom = "all", 
    contrast = c("HMP_BODY_SUBSITE", "Supragingival Plaque", "Subgingival Plaque"))

my_MAST <- set_MAST(
    pseudo_count = FALSE,
    rescale = "median",
    design = "~ 1 + RSID + HMP_BODY_SUBSITE",
    coefficient = "HMP_BODY_SUBSITESupragingival Plaque")

my_dearseq <- set_dearseq(
    pseudo_count = FALSE, 
    covariates = NULL,
    variables2test = "HMP_BODY_SUBSITE",
    sample_group = "RSID", 
    test = "asymptotic", 
    preprocessed = FALSE)

# Very time consuming
my_ANCOM <- set_ANCOM(
    pseudo_count = FALSE,
    adj_formula = NULL,
    rand_formula = "~ 1 | RSID",
    lme_control = list(maxIter = 100, msMaxIter = 100, opt = "optim"),
    contrast = c("HMP_BODY_SUBSITE",
    "Supragingival Plaque", "Subgingival Plaque"),
    BC = FALSE)

my_methods_noWeights <- c(
    my_edgeR_noWeights, 
    my_DESeq2_noWeights, 
    my_limma_noWeights,  
    my_ALDEx2,
    my_MAST,
    my_dearseq,
    my_ANCOM)
```

Similarly, to set the normalization methods, the `setNormalizations()` function can be used. In this case it has already been set up for the TIEC analysis.

```{r info_normalizations}
str(my_normalizations)
```

The `runSplits()` function generates the subsets and performs DA analysis on the features with at least 1 (min_counts \> 0) count in more than 2 samples (min_samples \> 2):

```{r runSplits}
# Make sure the subject ID variable is a factor
phyloseq::sample_data(ps_plaque_16S)[, "RSID"] <- as.factor(
    phyloseq::sample_data(ps_plaque_16S)[["RSID"]])

Plaque_16S_splitsDA <- runSplits(split_list = my_splits, 
    method_list = my_methods_noWeights, normalization_list = my_normalizations, 
    object = ps_plaque_16S, min_counts = 0, min_samples = 2, verbose = FALSE)
```

The structure of the output in this example is the following:

-   *Subset1* and *Subset2* on the first level, which contains:

    -   *Comparison1* to *Comparison2* output lists on the second level:

        -   results of 7 methods on the third level: `edgeR` with *TMM* scaling factors, `DESeq2` with *poscounts* normalization factors, `limma-voom` with *TMM* scaling factors (all the 3 previous methods have the Subject identifier in the design formula), `ALDEx2` with paired *wilcox* test and denom equals to *all*, `MAST` with median scaling and the subject identifier in the design formula, `dearseq` for repeated measures with *asymptotic* test, and `ANCOM` without bias correction (since it only allows for cross-sectional data). They are organized as already described in the "Goodness of Fit" chapter:

            -   `pValMat` which contains the matrix of raw p-values and adjusted p-values in *rawP* and *adjP* columns respectively;

            -   `statInfo` which contains the matrix of summary statistics for each feature, such as the *logFC*, standard errors, test statistics and so on;

            -   `dispEsts` which contains the dispersion estimates for methods like *edgeR* and *DESeq2*;

            -   `name` which contains the complete name of the used method.

It may happen that at a later time the user wants to add to the results already obtained, the results of another group of methods. First of all, the same splits and the same object must be used to obtain the new results:

```{r set_basic_methods}
my_basic <- set_basic(
    pseudo_count = FALSE, 
    contrast = c("HMP_BODY_SUBSITE",
    "Supragingival Plaque", "Subgingival Plaque"), 
    test = "wilcox", 
    paired = TRUE)

Plaque_16S_splitsDA_basic <- runSplits(split_list = my_splits, 
    method_list = my_basic, normalization_list = NULL, object = ps_plaque_16S,
    min_counts = 0, min_samples = 2, verbose = FALSE)
```

To put everything together we can use two nested `mapply`s and exploit the output structures:

```{r add_second_round}
Plaque_16S_splitsDA_all <- mapply(
    Plaque_16S_splitsDA, 
    Plaque_16S_splitsDA_basic, 
    FUN = function(splits1, splits2){
        mapply(splits1, splits2, FUN = function(group1, group2){
            return(c(group1, group2))
        }, SIMPLIFY = FALSE)
    }, SIMPLIFY = FALSE)
```

For each pair of methods the concordance is computed by the `createConcordance()` function. It produces a long format `data.frame` object with several columns:

-   *comparison* which indicates the comparison number;
-   *n_features* which indicates the total number of taxa in the comparison dataset;
-   name of *method1*;
-   name of *method2*;
-   *rank*;
-   *concordance* which is defined as the cardinality of the intersection of the top *rank* elements of each list, divided by *rank*, i.e., $\frac{L_{1:rank} \bigcap M_{1:rank}}{rank}$, where *L* and *M* represent the lists of p-values of *method1* and *method2* respectively. A noise value ($<10^{-10}$) is added to each p-value (or statistic) in order to avoid duplicated values which can not be ordered.

```{r createConcordance}
concordance <- createConcordance(
    object = Plaque_16S_splitsDA_all,
    slot = "pValMat",
    colName = "rawP",
    type = "pvalue"
)

head(concordance)
```

The `createConcordance()` method is very flexible. In the example below the concordances are built using the log fold changes or other statistics instead of the p-values. To do so, it is necessary to know the column names generated by each differential abundance method in the `statInfo` matrix.

Firstly, inspect the method order:

```{r getNames}
names(Plaque_16S_splitsDA_all$Subset1$Comparison1)
```

Then, look at the column names:

```{r logFC_names}
cat("edgeR.TMM", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$edgeR.TMM$statInfo)
cat("DESeq2.poscounts", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$DESeq2.poscounts$statInfo)
cat("limma.TMM", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$limma.TMM$statInfo)
cat("ALDEx2.all.wilcox.paired", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$ALDEx2.all.wilcox.paired$
    statInfo)
cat("MAST.median", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$MAST.median$statInfo)
cat("dearseq.repeated.asymptotic", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$dearseq.repeated.asymptotic$
    statInfo)
cat("ANCOM", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$ANCOM$statInfo)
cat("basic.wilcox.paired", "\n")
names(Plaque_16S_splitsDA_all$Subset1$Comparison1$basic.wilcox.paired$statInfo)
```

All methods, except for `DESeq2`, `ALDEx2`, `dearseq`, and `ANCOM`, contain the log fold change values in the *logFC* column of `statInfo` matrix. Knowing this, the concordance `data.frame` can be built using:

```{r alternativeConcordance, eval=FALSE}
concordance_alternative <- createConcordance(
    object = Plaque_16S_splitsDA_all,
    slot = "statInfo",
    colName = c("logFC", "log2FoldChange", "logFC", "effect", "logFC", "rawP", 
        "W", "logFC"),
    type = c("logfc", "logfc", "logfc", "logfc", "logfc", "pvalue", "logfc",
        "logfc")
)
```

## Visualization

Starting from the table of concordances, the `plotConcordance()` function can produce 2 graphical results visible in Figure \@ref(fig:plotConcordance):

-   the dendrogram of methods, clustered by the area over the concordance bisector in `concordanceDendrogram` slot;

-   the heatmap of the between and within method concordances in `concordanceHeatmap` slot. For each tile of the symmetric heatmap, which corresponds to a pair of methods, the concordance from rank 1 to a threshold rank is drawn.

The area between the curve and the bisector is colored to highlight concordant methods (blue) and non-concordant ones (red). The two graphical results should be drawn together for the best experience.

```{r plotConcordance, fig.cap="BMC and WMC plot. Between-method concordance (BMC) and within-method concordance (WMC) (main diagonal) averaged values from rank 1 to 30."}
pC <- plotConcordance(concordance = concordance, threshold = 30)
cowplot::plot_grid(plotlist = pC, ncol = 2, align = "h", axis = "tb",
    rel_widths = c(1, 3))
```

The WMC and BMC from rank 1 to rank 30 are reported in the plot above. More than 40 (use `table(concordance$rank)` to find out) is the maximum rank obtained by all split comparisons, *i.e.* the number of taxa for which all methods have been able to calculate p-values (in all comparisons). However, a custom threshold of 30 was supplied .

It is common that WMC values (in red rectangles) are lower than BMC ones. Indeed, BMC is computed between different methods on the same data, while WMC is computed for a single method, run in different datasets (some taxa are dataset-specific).

`limma.CSSmedian` and `limma.TMM` methods show the highest BMC values. Regarding the WMC, `MAST.median` has the lowest values, maybe because it has not been implemented properly for repeated measure designs. Also `ANCOM` shows low values, maybe because the statistic chosen as p-value (in the *pValMat* slot) is in reality computed as $1-W/(ntaxa-1)$ which is not a p-value. Other methods have comparable WMC values.

# Enrichment analysis

While mock comparisons and random splits allow to evaluate type I error and concordance, these analyses do not assess the correctness of the discoveries. Even the method with the highest WMC could nonetheless consistently identify false positive DA taxa.

While the lack of ground truth makes it challenging to assess the validity of DA results in real data, enrichment analysis can provide an alternative solution to rank methods in terms of their ability to identify, as significant, taxa that are known to be differentially abundant between two groups.

The relationships between the functions used in this section are explained by the diagram in Figure \@ref(fig:figenrichment).

```{r figenrichment, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.cap="Enrichment analysis diagram."}
knitr::include_graphics("./enrichment_structure.svg")
```

## *A priori* knowledge

Here, we leveraged the peculiar environment of the gingival site: the supragingival biofilm is directly exposed to the open atmosphere of the oral cavity, favoring the growth of aerobic species. In the subgingival biofilm, however, the atmospheric conditions gradually become strict anaerobic, favoring the growth of anaerobic species [@plaque_dynamics]. From the comparison of the two sites, we thus expected to find an abundance of aerobic microbes in the supragingival plaque and of anaerobic bacteria in the subgingival plaque. DA analysis should reflect this difference by finding an enrichment of aerobic (anaerobic) bacteria among the DA taxa with a positive (negative) log-fold-change.

Firstly, the microbial metabolism information is necessary. These data comes from [@cigarettes] paper github repository (<https://github.com/waldronlab/nychanesmicrobiome>), but they can be loaded using `data("microbial_metabolism")`.

```{r priorKnowledge}
data("microbial_metabolism")
head(microbial_metabolism)
```

The microbial genus and its type of metabolism are specified in the first and second column respectively. To match each taxon of the phyloseq object to its type of metabolism the next chunk of code can be used.

```{r exampleOfIntegration}
# Extract genera from the phyloseq tax_table slot
genera <- tax_table(ps_plaque_16S)[, "GENUS"]
# Genera as rownames of microbial_metabolism data.frame
rownames(microbial_metabolism) <- microbial_metabolism$Genus
# Match OTUs to their metabolism
priorInfo <- data.frame(genera, "Type" =  microbial_metabolism[genera, "Type"])
unknown_metabolism <- is.na(priorInfo$Type)
priorInfo[unknown_metabolism, "Type"] <- "Unknown"
# Relabel 'F Anaerobic' to 'F_Anaerobic' to remove space
priorInfo$Type <- factor(priorInfo$Type, 
    levels = c("Aerobic","Anaerobic","F Anaerobic","Unknown"), 
    labels = c("Aerobic","Anaerobic","F_Anaerobic","Unknown"))
# Add a more informative names column
priorInfo[, "newNames"] <- paste0(rownames(priorInfo), "|",
    priorInfo[, "GENUS"])
```

## Differential abundance

Both the normalization/scaling factors and the DA methods' instructions are available since the dataset is the same used in the previous section.

To add the normalization/scaling factors to the object:

```{r normalize_plaque}
ps_plaque_16S <- runNormalizations(my_normalizations, object = ps_plaque_16S)
```

A simple filter to remove rare taxa:

```{r filter_plauque}
ps_plaque_16S <- phyloseq::filter_taxa(physeq = ps_plaque_16S, 
    flist = function(x) sum(x > 0) >= 3, prune = TRUE)
ps_plaque_16S
```

Differently from the Type I Error Control and Concordance analyses, the enrichment analysis rely on a single `phyloseq` or `TreeSummarizedExperiment` object (no mocks, no splits, no comparisons). For this reason many methods can be assessed without computational trade-offs (e.g. `ANCOM` without sampling fraction bias correction and methods which use *ZINB* weights).

To compute observational weights:

```{r plaque_weights}
plaque_weights <- weights_ZINB(object = ps_plaque_16S, design = ~ 1,
    zeroinflation = TRUE)
```

We add to the existing methods, methods with observational weights:

```{r enrichment_methods}
my_edgeR <- set_edgeR(
    group_name = "HMP_BODY_SUBSITE", 
    design = ~ 1 + RSID + HMP_BODY_SUBSITE, 
    coef = "HMP_BODY_SUBSITESupragingival Plaque", 
    norm = "TMM", 
    weights_logical = TRUE)

my_DESeq2 <- set_DESeq2(
    contrast = c("HMP_BODY_SUBSITE",
    "Supragingival Plaque", "Subgingival Plaque"), 
    design = ~ 0 + RSID + HMP_BODY_SUBSITE, 
    norm = "poscounts",
    weights_logical = TRUE)

my_limma <- set_limma(
    design = ~ 1 + RSID + HMP_BODY_SUBSITE, 
    coef = "HMP_BODY_SUBSITESupragingival Plaque", 
    norm = "TMM",
    weights_logical = TRUE)

my_methods <- c(my_methods_noWeights, my_edgeR, my_DESeq2, my_limma)
```

All the ingredients are ready:

```{r runDA_enrichment, message=FALSE}
# Convert to factor
sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE <- 
    factor(sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE)
# Reference level = "Subgingival Plaque"
sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE <- relevel(
    x = sample_data(ps_plaque_16S)$HMP_BODY_SUBSITE,
    ref = "Subgingival Plaque"
)
# Make sure the subject ID variable is a factor
phyloseq::sample_data(ps_plaque_16S)[, "RSID"] <- as.factor(
    phyloseq::sample_data(ps_plaque_16S)[["RSID"]])

Plaque_16S_DA <- runDA(method_list = my_methods, 
    object = ps_plaque_16S, weights = plaque_weights, verbose = FALSE)
```

## Visualization

`Plaque_16_DA` object contains the results for 10 methods. In order to extract p-values, the optional direction of DA (DA vs non-DA, or UP Abundant vs DOWN Abundant), and to add any *a priori* information, the `createEnrichment()` function can be used.

In the *direction* argument, which is set to `NULL` by default, the column name containing the direction (e.g. *logfc*, *logFC*, *logFoldChange*...) of each method's `statInfo` matrix must be supplied.

Firstly, the order of methods should be investigated:

```{r info_DA}
names(Plaque_16S_DA)
```

Following the methods' order, the *direction* parameter is supplied together with other parameters:

-   *threshold_pvalue*, *threshold_logfc*, and *top* (optional), to set differential abundance thresholds;

-   *slot*, *colName*, and *type*, which specify where to apply the above thresholds;

-   *priorKnowledge*, *enrichmentCol*, and *namesCol*, to add enrichment information to DA analysis;

We use the `createEnrichment()` with the direction parameter for all method except `ANCOM` and `dearseq` (the first has the *W* statistic which is only positive, while the second has only p-values).

```{r createEnrichment}
enrichment <- createEnrichment(
    object = Plaque_16S_DA[-c(6:7)], 
    priorKnowledge = priorInfo, 
    enrichmentCol = "Type",
    namesCol = "newNames", 
    slot = "pValMat", 
    colName = "adjP", 
    type = "pvalue",
    direction = c(
        "logFC", # edgeR
        "log2FoldChange", # DEseq2
        "logFC", # limma
        "effect", # ALDEx2
        "logFC", # MAST
        "logFC", # edgeR with weights
        "log2FoldChange", # DESeq2 with weights
        "logFC"), # limma with weights
    threshold_pvalue = 0.1,
    threshold_logfc = 0,
    top = NULL,
    alternative = "greater",
    verbose = TRUE
)
```

The produced *enrichment* object consists in a list of elements as long as the number of tested methods:

-   the `data` slot contains information for each feature. P-values, adjusted p-values (or other statistics) in *stats* column, log fold changes (or other statistics, if specified) in *direction* column, differential abundance information in the *DA* column (according to the thresholds), the variable of interest for the enrichment analysis, and the name of the feature in the *feature* column;

-   in the `tables` slot a maximum of *2 x (levels of enrichment variable)* contingency tables (*2x2*) are present;

-   in the `tests` slot, the list of Fisher exact tests produced by the `fisher.test()` function are saved for each contingency table;

-   in the `summaries` slot, the first elements of the contingency tables and the respective p-values are collected for graphical purposes.

Considering one of the methods, `DESeq2.poscounts`, we obtain 8 contingency tables. Both UP Abundant and DOWN Abundant taxa are found and the enrichment variable has Aerobic, Anaerobic, F_Anaerobic, and Unknown levels. For each level, we can build 2 contingency tables: one for DOWN Abundant vs non-DOWN Abundant features and one for UP Abundant vs non-UP Abundant features. The enrichment is tested using Fisher exact test. The `plotContingency()` function summarize all these information (Figure \@ref(fig:plotContingency)).

```{r plotContingency, fig.cap="Contingency tables plot. Contingency tables for Aerobic and Anaerobic taxa found as differentially abundant by DESeq2.poscounts DA method. Fisher exact test has been performed on each contingency table. If the enrichment is signficantly present, the correspondent cell will be highlighted."}
plotContingency(enrichment = enrichment, 
    levels_to_plot = c("Aerobic", "Anaerobic"), 
    method = "DESeq2.poscounts")
```

To summarize enrichment analysis for all the methods simultaneously, the `plotEnrichment()` function can be used. Only Aerobic and Anaerobic levels are plotted in Figure \@ref(fig:plotEnrichment):

```{r plotEnrichment, fig.width=7, fig.height=7, fig.cap="Enrichment plot. Number of differentially abundant features, colored by aerobic or anaerobic metabolism, and directed according to differential abundance direction (UP or DOWN abundant)."}
plotEnrichment(enrichment = enrichment, enrichmentCol = "Type", 
    levels_to_plot = c("Aerobic", "Anaerobic"))
```

Since "Subgingival Plaque" is the reference level for each method, the coefficients extracted from the methods are referred to the "Supragingival Plaque" class. Five out of eight methods identify, as expected, a statistically significant ($0.001 < p \le 0.05$) amount of DOWN Abundant Anaerobic features in Supragingival Plaque (Figure \@ref(fig:plotEnrichment)). Moreover, all of them find and enriched amount of UP Abundant Aerobic genera in Supragingival Plaque. Both `DESeq2.poscounts` and `DESeq2.poscounts.weighted` find many Anaerobic genera as UP Abundant, which are not expected and could be false positives, probably.

To investigate the DA features, the `plotMutualFindings()` function can be used (Figure \@ref(fig:plotMutualFindings)). While *levels_to_plot* argument allows to choose which levels of the enrichment variable to plot, *n_methods* argument allows to extract only features which are mutually found as DA by more than 1 method.

```{r plotMutualFindings, fig.width=6, fig.height=6, fig.cap="Mutual Findings plot. Number of differentially abundant features mutually found by 1 or more methods, colored by the differential abundance direction and separated by aerobic and anaerobic metabolism."}
plotMutualFindings(enrichment, enrichmentCol = "Type", 
    levels_to_plot = c("Aerobic", "Anaerobic"), n_methods = 1)
```

In this example (Figure \@ref(fig:plotMutualFindings)), many Anaerobic genera and 6 Aerobic genera are found as DA by more than 1 method simultaneously. Among them *Prevotella*, *Treponema*, *Fusobacterium*, *Catonella*, and *Dialister* genera are found as DOWN Abundant in Supragingival Plaque by all methods. While the *Actinomyces* genus is considered UP Abundant in Supragingival Plaque .

## True and False Positives

To evaluate the overall performances we can study a statistic based on the difference between putative True Positives (TP) and the putative False Positives (FP). To build the matrix to plot, the `createPositives()` can be used. In details, the correctness of the DA features is evaluated comparing the direction of the top ranked features to the expected direction supplied by the user in the *TP* and *FP* lists. The procedure is performed for several thresholds of *top* parameter in order to observe a trend, if present.

```{r createPositives}
positives <- createPositives(
    object = Plaque_16S_DA[-c(6,7)], 
    priorKnowledge = priorInfo, 
    enrichmentCol = "Type",
    namesCol = "newNames", 
    slot = "pValMat", 
    colName = "rawP", 
    type = "pvalue",
    direction = c(
        "logFC", # edgeR
        "log2FoldChange", # DEseq2
        "logFC", # limma
        "effect", # ALDEx2
        "logFC", # MAST
        "logFC", # edgeR with weights
        "log2FoldChange", # DESeq2 with weights
        "logFC"), # limma with weights
    threshold_pvalue = 1,
    threshold_logfc = 0,
    top = seq.int(from = 0, to = 40, by = 5),
    alternative = "greater",
    verbose = FALSE,
    TP = list(c("DOWN Abundant", "Anaerobic"), c("UP Abundant", "Aerobic")),
    FP = list(c("DOWN Abundant", "Aerobic"), c("UP Abundant", "Anaerobic"))
)
head(positives)
```

Finally, the `plotPositives()` function can be used to summarize the methods' performances (Figure \@ref(fig:plotPositives)). Higher values usually represents better performances. In our example, all methods show similar values of the statistics from the top 5, to the top 20 ranked features.

```{r plotPositives, fig.cap="TP, FP differences plot. Differences between the number of True Positives and False Positives for several thresholds of the top ranked raw p-values (the top 5 lowest p-values, the top 10, 15, ..., 50) for each method."}
plotPositives(positives)
```

`MAST.median` and `ALDEx2.all.wilcox.paired` methods are very conservative and are located on the lower part of the Figure \@ref(fig:plotPositives). More liberal methods, such as `limma.TMM.weighted` and `edgeR.TMM` show instead the highest values. This means that their findings are in line with the *a priori* knowledge supplied by the user, however we are considering a toy example with only 58 features.

## Enrichment without direction

When the user have a custom method where the direction of the differential abundance is not returned (*e.g.* NOISeq), or when the direction of DA is not of interest, the sole information about DA and not DA feature can be used. We use the `createEnrichment()` without the direction parameter for all methods.

```{r createEnrichment_nodir}
enrichment_nodir <- createEnrichment(
    object = Plaque_16S_DA, 
    priorKnowledge = priorInfo, 
    enrichmentCol = "Type",
    namesCol = "newNames", 
    slot = "pValMat", 
    colName = "adjP", 
    type = "pvalue",
    threshold_pvalue = 0.1,
    threshold_logfc = 0,
    top = NULL,
    alternative = "greater",
    verbose = FALSE
)
```

To summarize enrichment analysis for all the methods simultaneously, the `plotEnrichment()` function can be used. All levels are plotted in Figure \@ref(fig:plotEnrichmentnodir).

```{r plotEnrichmentnodir, fig.width=7, fig.height=7, fig.cap="Enrichment plot. Number of differentially abundant features, colored by metabolism."}
plotEnrichment(enrichment = enrichment_nodir, enrichmentCol = "Type")
```

The highest amount of DA features belongs to the *Anerobic* metabolism, followed by *F_Anaerobic*, and *Aerobic*. The method that finds more DA features is `DESeq2.poscounts`, while `ANCOM` is the most conservative

As for enrichment analysis with DA direction, the `plotMutualFindings()` function can be used here too (Figure \@ref(fig:plotMutualFindingsnodir)). While *levels_to_plot* argument allows to choose which levels of the enrichment variable to plot, *n_methods* argument allows to extract only features which are mutually found as DA by more than 1 method.

```{r plotMutualFindingsnodir, fig.width=6, fig.height=6, fig.cap="Mutual Findings plot. Number of differentially abundant features mutually found by 1 or more methods, separated by aerobic and anaerobic metabolism."}
plotMutualFindings(enrichment_nodir, enrichmentCol = "Type", 
    levels_to_plot = c("Aerobic", "Anaerobic"), n_methods = 1)
```

In this example (Figure \@ref(fig:plotMutualFindingsnodir)), only a *Prevotella* OTU is found as DA in Supragingival Plaque by all methods (probably because `ANCOM` p-value threshold is not interpretable as the same threshold for the other methods).

## Enrichment analysis for simulated data

To enlarge the scope of the enrichment analysis, simulations could be used, e.g. by using the user's dataset as a template to generate simulated data, in which to know the DA features and provide this information as prior knowledge.

As an example, here we leverage the `SPsimSeq` package (the tool to use is up to the user) to simulate only a single dataset (`n.sim = 1`) from the `ps_plaque_16S` dataset where two body sub sites are available (without considering the paired design). The data are simulated with the following properties - 100 features (`n.genes = 100`) - 50 samples (`tot.samples = 50`) - the samples are equally divided into 2 groups each with 25 samples (`group.config = c(0.5, 0.5)`) - all samples are from a single batch (`batch.config = 1`) - we add 20% DA features (`pDE = 0.2`) - the DA features have a log-fold-change of at least 0.5 - we do not model the zeroes separately (`model.zero.prob = FALSE`).

```{r simulate_plaques}
data("ps_plaque_16S")
counts_and_metadata <- get_counts_metadata(ps_plaque_16S)
plaque_counts <- counts_and_metadata[["counts"]]
plaque_metadata <- counts_and_metadata[["metadata"]]

set.seed(123)

sim_list <- SPsimSeq(
    n.sim = 1, 
    s.data = plaque_counts,
    group = plaque_metadata[, "HMP_BODY_SUBSITE"],
    n.genes = 100, 
    batch.config = 1,
    group.config = c(0.5, 0.5), 
    tot.samples = 50, 
    pDE = 0.2, 
    lfc.thrld = 0.5, 
    model.zero.prob = FALSE,
    result.format = "list")
```

Simulated data are organised into a `TreeSummarizedExperiment` object:

```{r simulated_to_TSE}
library(TreeSummarizedExperiment)

sim_obj <- TreeSummarizedExperiment::TreeSummarizedExperiment(
    assays = list("counts" = sim_list[[1]][["counts"]]),
    rowData = sim_list[[1]]["rowData"],
    colData = sim_list[[1]]["colData"],
)
# Group as factor
SummarizedExperiment::colData(sim_obj)[, "colData.Group"] <- as.factor(
    SummarizedExperiment::colData(sim_obj)[, "colData.Group"])
```

The *apriori* informations are readily available from the `sim_list[[1]]["rowData"]`.

```{r priorInfo_simulated}
priorInfo <- sim_list[[1]][["rowData"]]
priorInfo$Reality <- ifelse(priorInfo[, "DE.ind"], "is DA", "is not DA")
```

Once again we compute the normalization/scaling factors:

```{r normalizing_simulated}
sim_obj <- runNormalizations(normalization_list = my_normalizations, 
    object = sim_obj, verbose = TRUE)
```

We filter rare taxa:

```{r filter_simulated}
taxa_to_keep <- apply(assays(sim_obj)[["counts"]], 1, function(x) 
        sum(x > 0) >= 3)
sim_obj <- sim_obj[taxa_to_keep, ]
priorInfo <- priorInfo[taxa_to_keep, ]
```

We compute weights:

```{r weights_simulated}
sim_weights <- weights_ZINB(object = sim_obj, design = ~ 1, 
    zeroinflation = TRUE)
```

We set the methods' instructions:

```{r set_methods_simulated}
my_basic <- set_basic(pseudo_count = FALSE,
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"), 
    test = c("t", "wilcox"), 
    paired = FALSE, 
    expand = TRUE)

my_edgeR <- set_edgeR(
    pseudo_count = FALSE,
    group_name = "colData.Group",
    design = ~ colData.Group,
    robust = FALSE,
    coef = 2,
    norm = "TMM",
    weights_logical = c(TRUE, FALSE),
    expand = TRUE)

my_DESeq2 <- set_DESeq2(
    pseudo_count = FALSE,
    design = ~ colData.Group,
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"),
    norm = "poscounts",
    weights_logical = c(TRUE, FALSE),
    alpha = 0.05,
    expand = TRUE)

my_limma <- set_limma(
    pseudo_count = FALSE,
    design = ~ colData.Group,
    coef = 2,
    norm = "TMM",
    weights_logical = c(FALSE, TRUE),
    expand = TRUE)

my_ALDEx2 <- set_ALDEx2(
    pseudo_count = FALSE,
    design = "colData.Group",
    mc.samples = 128,
    test = "wilcox",
    paired.test = FALSE, 
    denom = c("all", "iqlr"), 
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"),
    expand = TRUE)

my_metagenomeSeq <- set_metagenomeSeq(
    pseudo_count = FALSE, 
    design = "~ colData.Group", 
    coef = "colData.GroupSupragingival Plaque", 
    norm = "CSS", 
    model = "fitFeatureModel",
    expand = TRUE)

my_corncob <- set_corncob(
    pseudo_count = FALSE,
    formula = ~ colData.Group,
    formula_null = ~ 1,
    phi.formula = ~ colData.Group,
    phi.formula_null = ~ colData.Group,
    test = c("Wald", "LRT"),
    boot = FALSE, 
    coefficient = "colData.GroupSupragingival Plaque")

my_MAST <- set_MAST(
    pseudo_count = FALSE,
    rescale = c("default", "median"),
    design = "~ 1 + colData.Group",
    coefficient = "colData.GroupSupragingival Plaque",
    expand = TRUE)

my_Seurat <- set_Seurat(
    pseudo_count = FALSE,
    test = c("t", "wilcox"),
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"),
    norm = c("LogNormalize", "CLR"),
    scale.factor = 10^5,
    expand = TRUE
)

my_ANCOM <- set_ANCOM(
    pseudo_count = FALSE,
    formula = "colData.Group", 
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"),
    BC = c(TRUE, FALSE), 
    expand = TRUE   
)

my_dearseq <- set_dearseq(
    pseudo_count = FALSE,
    covariates = NULL,
    variables2test = "colData.Group", 
    preprocessed = FALSE, 
    test = c("permutation", "asymptotic"),
    expand = TRUE)

my_NOISeq <- set_NOISeq(
    pseudo_count = FALSE, 
    contrast = c("colData.Group", "Supragingival Plaque", 
        "Subgingival Plaque"),
    norm = c("rpkm", "tmm"), 
    expand = TRUE)

my_methods <- c(my_basic, my_edgeR, my_DESeq2, my_limma, my_metagenomeSeq,
    my_corncob, my_ALDEx2, my_MAST, my_Seurat, my_ANCOM, my_dearseq, my_NOISeq)
```

We run the methods:

```{r runDA_simulated}
sim_DA <- runDA(method_list = my_methods, 
    object = sim_obj, weights = sim_weights, verbose = FALSE)
```

We use the `createEnrichment()` without the direction parameter for all methods using 0.1 as threshold for the adjusted p-values in order to define DA and non-DA taxa.

```{r createEnrichment_nodir_sim}
enrichment_nodir <- createEnrichment(
    object = sim_DA, 
    priorKnowledge = priorInfo, 
    enrichmentCol = "Reality",
    namesCol = NULL, 
    slot = "pValMat", 
    colName = "adjP", 
    type = "pvalue",
    threshold_pvalue = 0.1,
    threshold_logfc = 0,
    top = NULL,
    alternative = "greater",
    verbose = FALSE
)
```

To summarize enrichment analysis for all the methods simultaneously, the `plotEnrichment()` function can be used. Both the numbers of "is DA" and "is not DA" features are plotted in Figure \@ref(fig:plotEnrichmentnodirsim). Their interpretation is quite straightforward: "is DA" are the true positive findings, while the "is not DA" are the false positives.

```{r plotEnrichmentnodirsim, fig.width=7, fig.height=7, fig.cap="Enrichment plot for simulated data. Number of differentially abundant features, colored by DA reality."}
plotEnrichment(enrichment = enrichment_nodir, enrichmentCol = "Reality")
```

From this example, only 9 out of 25 methods are able to find an enriched amount of truly DA features without any false discovery: `ANCOM.counts.BC` and `dearseq.counts.permutation.1000` in the first position.

To further assess methods' power, the `createPositives()` function can be used specifying as True Positives the resulting DA features created as real DA features and as False Positives the resulting DA features created as not DA features (figure \@ref(fig:plotPositivessim)).

We use a *threshold_pvalue = 0.1* to call a feature DA based on its adjusted p-value. We compute the difference between TPs and FPs for several *top* thresholds (from 5 to 30, by 5) in order to observe a trend.

```{r createPositives_sim}
positives_nodir <- createPositives(
    object = sim_DA, 
    priorKnowledge = priorInfo, 
    enrichmentCol = "Reality",
    namesCol = NULL, 
    slot = "pValMat", 
    colName = "adjP", 
    type = "pvalue",
    threshold_pvalue = 0.1,
    threshold_logfc = 0,
    top = seq(5, 30, by = 5),
    alternative = "greater",
    verbose = FALSE,
    TP = list(c("DA", "is DA")),
    FP = list(c("DA", "is not DA"))
)
```

Since we simulated 20 DA features, we know that the maximum number of TPs is 20.

```{r plotPositivessim, fig.cap="TP, FP differences plot. Differences between the number of True Positives and False Positives for several thresholds of the top ranked adjusted p-values lower than 0.1 (the top 5 lowest adjusted p-values, the top 10, 15, ..., 30) for each method in simulated data. Red dotted line represents the total number of DA simulated features.", fig.width=8, fig.height=8}
plotPositives(positives = positives_nodir) +
    facet_wrap( ~ method) + 
    theme(legend.position = "none") +
    geom_hline(aes(yintercept = 20), linetype = "dotted", color = "red") +
    geom_hline(aes(yintercept = 0), color = "black") +
    ylim(NA, 21)
```

From figure \@ref(fig:plotPositivessim) it is clearly visible that `ANCOM.counts.BC` and `dearseq.counts.permutation.1000`, `dearseq.counts.asymptotic` reach the highest values of the difference, followed by `corncob.counts.Wald` and `DESeq2.counts.poscounts`. As already mentioned the desired level of power which a methods should be able to reach is represented by the red dotted line, *i.e.* the total number of DA simulated features (20 in our case). These methods, in this specific example, have the highest power. Differently, methods characterized by flat lines have a fixed number of features with an adjusted p-value lower than the threshold. If their lines are above the zero line, it means that the number of True Positives is greater than the number of False Positives. On the contrary, if their lines are below the zero line, it means that the number of False Positives is greater (*e.g.* `edgeR.counts.TMM.weighted` has negative values, maybe due to poor weight estimates).

# Session Info

```{r sessionInfo}
sessionInfo()
```

# References
